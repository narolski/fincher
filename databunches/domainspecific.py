from fastai.basic_data import DataBunch
from fastai.text import *
from fastai import *
import pandas as pd

class DomainSpecificDataBunch:
    """
    Creates and manages a DomainSpecificDataBunch.
    """

    def __init__(self, wikipedia_folder_path: Path, domain_specific_df: pd.DataFrame, data_path: Path,
                 text_column_name: str =
    "review", validation_split: float = 0.1, batch_size: int = 64):
        """
        Creates a DomainSpecificDataBunch, performing a sentencepiece-based tokenization on an entire dataset.
        :param wikipedia_folder_path: path to folder containing the previously downloaded Wikipedia contents
        :param domain_specific_df: DataFrame containg the contents of an unlabelled, domain-specific dataset
        :param text_column_name: name of the column containing the text in a DataFrame
        :param validation_split: an amount of data to randomly put in a validation set from a DataFrame
        :param batch_size: batch size to use during training
        :param data_path: path where to store files generated by DataBunch
        """
        if validation_split <= 0 or validation_split >= 1:
            raise Exception("Invalid validation split provided")

        self.data_lm = (TextList.from_df(domain_specific_df, data_path, cols=text_column_name, processor=[SPProcessor.load(
            wikipedia_folder_path/'docs')])
                   .split_by_rand_pct(validation_split, seed=42)
                   .label_for_lm()
                   .databunch(bs=batch_size, num_workers=1))

    def save(self, path):
        """
        Saves the DomainSpecificDataBunch to a given path.
        :param path: path where DomainSpecificDataBunch is to be saved
        :return:
        """
        self.data_lm.save(path)

    def get_databunch(self) -> DataBunch:
        """
        Returns a DataBunch object.
        :param path:
        :return: DataBunch
        """
        return self.data_lm
